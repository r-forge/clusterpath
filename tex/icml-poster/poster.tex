\documentclass[final]{beamer} 
\usetheme[headheight=10cm,footheight=5cm]{boxes}
\usetheme{toby}
\usepackage{booktabs}
\usepackage{etex}
\usepackage{amsmath,amssymb}
\usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usepackage{helvet}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}   % e.g. for DIN-A0 poster
\usepackage{exscale}
\usepackage{pst-plot}
\usepackage{pstricks-add}
\usepackage{epsfig}
\usepackage{eulervm}
\usepackage{tikz}
\usepackage{array}
\usepackage{subfigure}
\usepackage{graphicx}
  
% \usepackage[size=custom,width=3.0,height=120,scale=2,debug]{beamerposter}  % e.g. custom size poster

\usepackage{beamerthemetoby}

\input{macros.tex}
\newcommand{\rred}[1]{{\textcolor{red}{#1}}}
\definecolorset{rgb}{}{}{inriablue,0.35,0.31,0.75}
\newcommand{\bblue}[1]{{\textcolor{inriablue}{#1}}}

\title[]{\veryHuge{Clusterpath: an Algorithm for Clustering using Convex Fusion Penalties}}
\author[]{Toby Dylan Hocking \and
      Armand Joulin \and
      Francis Bach \and
      Jean-Philippe Vert}
    \institute[Sierra]{~}%DONT DELETE THIS
\begin{document}
\begin{frame}{} 
\begin{columns}[T]
\hfill
\begin{column}{0.315\paperwidth}
% \begin{block}{One minute overview}
% \begin{itemize}
% \item Solving large-scale \rred{structured sparse} regularized problems.
% \item Use of (accelerated) \rred{proximal gradient methods}.
% \item Proximal operator solved with \rred{network flow optimization}.
% \end{itemize}
% \end{block}
\begin{block}{The clustering problem: different appraoches}
\begin{minipage}{4in}
  Clustering: assign labels to $n$ points in $p$ dimensions
    $X\in\RR^{n\times p}$.
\end{minipage}
\hspace{1in}
\begin{minipage}{4in}
  Methods:
    \begin{itemize}
  \item K-means
  \item Hierarchical
  \item Mixture models
  \item Spectral (Ng \emph{et al.} 2001)
    \end{itemize}
\end{minipage}
\begin{minipage}{4in}
    Issues:
    \begin{itemize}
    \item Hierarchy
    \item Convexity
    \item Greediness
    \item Stability
    \item Interpretability 
  \end{itemize}
\end{minipage}
\end{block}


\begin{block}{Clusterpath: relaxing a hard fusion penalty}
\begin{itemize}
\item Hard-thresholding of differences is a combinatorial problem:
$
 \min_{    \alpha\in\RR^{n\times p}}       ||\alpha-X||_F^2 \text{  subject to  }
\sum_{i<j}1_{\alpha_i\neq\alpha_j} \leq t$
\item \alert{Relaxation:$\sum_{i<j}||\alpha_i-\alpha_j||_q w_{ij}\leq t$}
\item The Lagrange form is useful for optimization algorithms:
$$
\alpha^*(X,\lambda,q,w)=\operatorname{argmin}_{\alpha\in\RR^{n\times p}}
\frac 1 2||\alpha-X||_F^2+\lambda\sum_{i<j}||\alpha_i-\alpha_j||_q w_{ij}
$$ 
\item The \alert<1>{clusterpath} of $X$ is the path of optimal
  $\alpha^*$ obtained by varying $\lambda$, for fixed weights
  $w_{ij}\in\RR^+$ and norm $q\in\{1,2,\infty\}$.
\item 
 Related work: ``fused lasso'' Tibshirani and Saunders (2005),
``grouping pursuit'' Shen and Huang (2010), ``sum of norms'' Lindsten
\emph{et al.}  (2011).
\end{itemize} 
\end{block}
\input{cvx-allnorms}
\input{geometry} 
\vspace{-1in}
\begin{block}{We propose dedicated algorithms for each norm}
{\small
\begin{center}
\begin{tabular}{ccccc}
\hline
Norm & Properties & Algorithm &Complexity & Problem sizes \\
\hline
1 &piecewise linear, separable&path&$O(pn\log n)$&large $\approx 10^5$  \\
2 &rotation invariant&active-set& $O(n^2p)$&medium $\approx10^3$ \\
$\infty$ & piecewise linear&Frank-Wolfe&unknown*&medium $\approx 10^3$\\
\hline
\end{tabular}
\end{center}
*each iteration of complexity $O(n^2p)$.
}
\end{block}

\end{column}\hfill

\begin{column}{0.315\linewidth}

\begin{block}{Outline of the $\ell_1$ path algorithm}
Condition sufficient for optimality:
$$0 = \alpha_i - X_i + 
\lambda \sum_{j\neq i\atop \alpha_i \neq \alpha_j}w_{ij}
\operatorname{sign}({\alpha_i-\alpha_j}) + 
\lambda \sum_{j\neq i \atop \alpha_i = \alpha_j}w_{ij} \beta_{ij},$$
with $|\beta_{ij}|\leq 1$ and $\beta_{ij}=-\beta_{ji}$ (Hoefling 2009).
\begin{enumerate}
\item For $\lambda=0$ the solution $\alpha=X$ is optimal. We
  initialize the clusters $C_i = \{i\}$ and coefficients $\alpha_i =
  X_i$ for all $i$.
\item As $\lambda$ increases, the solutions will follow straight
  lines.
\item Taking the derivative of the optimality condition with respect
  to $\lambda$ and summing over all points in a cluster $C$ leads to:
$$\frac {d\alpha_C}{d\lambda}=v_C = \sum_{j\not\in C}w_{jC}\sign(\alpha_j-\alpha_C)=\sum_{j\not\in C}\sum_{i\in C} w_{ij}\sign(\alpha_j-\alpha_C)$$
\item When 2 clusters $C_1$ and $C_2$ fuse, they form a
  new cluster $C = C_1\cup C_2$ with $v_C = (
|C_1|v_1 + |C_2|v_2
)/(
|C_1|+|C_2|
)$.
\item Stop when all the points merge at the mean $\overline X$.
\item Combine dimensions using $\lambda$ values.
\end{enumerate}

\end{block}

\input{l1-2d}\input{l1-dims}

\begin{alertblock}{Free software!
    \textbf{\url{http://clusterpath.r-forge.r-project.org/}}}
\begin{itemize}
\item Dedicated C++ optimization algorithms with R interface.
  \begin{itemize}
  \item Calculates the exact $\ell_1$ clusterpath for identity weights.
  \item Active-set algorithm for the $\ell_1$ and $\ell_2$
    clusterpath with general weights.
  \end{itemize}
\item R interface to Python \texttt{cvxmod} clusterpath solver.
\item Clusterpath visualizations in 2d, 3d, and animations.
\item Coming soon: picking the number of clusters automatically!
\end{itemize}
\end{alertblock}

\begin{block}{Future work}
  \begin{itemize}
  \item Necessary and sufficient conditions for cluster splitting?
  \item Automatically learning weights and number of clusters?
  \item Applications to solving proximal problems.
  \end{itemize}
\end{block}
\end{column}\hfill
\begin{column}{0.315\linewidth}


\begin{block}{Clustering performance and timings}
\begin{itemize}
\item Cluster using the prior knowledge that there are 2 clusters.
\item Quantify partition correspondence using the Normalized Rand
  Index (Hubert and Arabie, 1985): 1 for perfect correspondence, 0 for
  completely random assignment.
\item Results for 2 non-convex interlocking half-moons in 2d:
\begin{center}
\begin{tabular}{lrrrr}
\hline
  Clustering method & Rand & SD & Seconds & SD \\ 
  \hline
  $e_{\exp}$ spectral clusterpath & 0.99 & 0.00 & 8.49 & 2.64 \\ 
  $e_{\exp}$ spectral kmeans & 0.99 & 0.00 & 3.10 & 0.08 \\ 
  $\ell_2$ clusterpath & 0.95 & 0.12 & 29.47 & 2.31 \\ 
  $e_{01}$ Ng et al. kmeans & 0.95 & 0.19 & 7.37 & 0.42 \\ 
  $e_{01}$ spectral kmeans & 0.91 & 0.19 & 3.26 & 0.21 \\ 
  Gaussian mixture & 0.42 & 0.13 & 0.07 & 0.00 \\
  average linkage & 0.40 & 0.13 & 0.05 & 0.00 \\ 
  kmeans & 0.26 & 0.04 & 0.01 & 0.00 \\ 
\hline
\end{tabular}
\end{center}
\item Similar performance to spectral clustering, and learns a tree:
\end{itemize}
\end{block}
\includegraphics[width=0.3\paperwidth]{moons}

The weighted $\ell_2$ clusterpath applied to the iris data:
\includegraphics{iris-clusterpath}
\input{moons-iris} 


\end{column}
\end{columns}

\end{frame}
\end{document}

