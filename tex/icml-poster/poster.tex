\documentclass[final]{beamer} 
\usetheme[headheight=10cm,footheight=5cm]{boxes}
\usetheme{toby}
%\usepackage{times}
\usepackage{etex}
\usepackage{amsmath,amssymb}
\usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usepackage{helvet}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}   % e.g. for DIN-A0 poster
\usepackage{exscale}
\usepackage{pst-plot}
\usepackage{pstricks-add}
\usepackage{epsfig}
\usepackage{eulervm}
\usepackage{tikz}
\usepackage{array}
\usepackage{subfigure}
\usepackage{graphicx}
  
% \usepackage[size=custom,width=3.0,height=120,scale=2,debug]{beamerposter}  % e.g. custom size poster

\usepackage{beamerthemetoby}

\input{macros.tex}
\newcommand{\rred}[1]{{\textcolor{red}{#1}}}
\definecolorset{rgb}{}{}{inriablue,0.35,0.31,0.75}
\newcommand{\bblue}[1]{{\textcolor{inriablue}{#1}}}

\title[]{\veryHuge{Clusterpath: an Algorithm for Clustering using Convex Fusion Penalties}}
\author[]{Toby Dylan Hocking \and
      Armand Joulin \and
      Francis Bach \and
      Jean-Philippe Vert}
    \institute[Sierra]{~}%DONT DELETE THIS
\begin{document}
\begin{frame}{} 
\begin{columns}[T]
\hfill
\begin{column}{0.315\paperwidth}
% \begin{block}{One minute overview}
% \begin{itemize}
% \item Solving large-scale \rred{structured sparse} regularized problems.
% \item Use of (accelerated) \rred{proximal gradient methods}.
% \item Proximal operator solved with \rred{network flow optimization}.
% \end{itemize}
% \end{block}
\begin{block}{The clustering problem: existing methods}
  \begin{itemize}
  \item Clustering: assign labels to $n$ points in $p$ dimensions
    $X\in\RR^{n\times p}$.
  \item Methods:
    \begin{itemize}
  \item K-means
  \item Hierarchical
  \item Mixture models
  \item Spectral (Ng \emph{et al.} 2001)
  \item ... 
    \end{itemize}
    \item Issues:
    \begin{itemize}
    \item Hierarchy
    \item Convexity
    \item Greediness
    \item Stability
  \end{itemize}
  \end{itemize}
\end{block}


\begin{block}{Clusterpath: a convex relaxation of hierarchical
    clustering}
\begin{itemize}
\item A hard-thresholding of differences is a combinatorial problem:
$
 \min_{    \alpha\in\RR^{n\times p}}       ||\alpha-X||_F^2 \text{  subject to  }
\sum_{i<j}1_{\alpha_i\neq\alpha_j} \leq t$
\item Relaxation:$\sum_{i<j}||\alpha_i-\alpha_j||_q w_{ij}\leq t$
\item The Lagrange form is useful for optimization algorithms:
$$
\alpha^*(\lambda,q,w)=\operatorname{argmin}_{\alpha\in\RR^{n\times p}}
\frac 1 2||\alpha-X||_F^2+\lambda\sum_{i<j}||\alpha_i-\alpha_j||_q w_{ij}
$$
\item The \alert<1>{clusterpath} the continuous path of optimal
  $\alpha^*$ obtained by varying $\lambda$, for fixed weights
  $w_{ij}\in\RR^+$ and norm $q\in\{1,2,\infty\}$.
\item
 Related work: ``fused lasso'' Tibshirani and Saunders (2005),
``grouping pursuit'' Shen and Huang (2010), ``sum of norms'' Lindsten
\emph{et al.}  (2011).
\end{itemize}


\end{block}


\end{column}\hfill

\begin{column}{0.315\linewidth}
\begin{block}{Hierarchical norms: Jenatton et al, ICML 2010} 
\begin{itemize}
   \item A set of groups is \rred{tree-structured} if $$\forall g,h \in \GG,~~ g \cap h = \emptyset ~~\text{or}~~ g \subset h ~~\text{or}~~ h \subset g.$$
   \item Order the groups $g_1,\ldots,g_m$ \rred{from the leaves to the root}.
   \item The proximal operator admits a \rred{closed form}: $$\text{Prox}_{\lambda\Omega} = \text{Prox}^{g_1} \circ \ldots \circ \text{Prox}^{g_m}.$$
   \item Sequence of small proximal operators/projections.
   \item \rred{$O(p)$ operations} for $\ell_2$-norms ($O(pd)$ for $\ell_\infty$-norms).
\end{itemize}
\end{block}

\begin{block}{Outline of the $\ell_1$ path algorithm}
$$0 = \alpha_i - X_i + 
\lambda \sum_{j\neq i\atop \alpha_i \neq \alpha_j}w_{ij}
\operatorname{sign}({\alpha_i-\alpha_j}) + 
\lambda \sum_{j\neq i \atop \alpha_i = \alpha_j}w_{ij} \beta_{ij},$$
with $|\beta_{ij}|\leq 1$ and $\beta_{ij}=-\beta_{ji}$ (Hoefling 2009).
\begin{enumerate}
\item For $\lambda=0$ the solution $\alpha=X$ is optimal. We
  initialize the clusters $C_i = \{i\}$ and coefficients $\alpha_i =
  X_i$ for all $i$.
\item As $\lambda$ increases, the solutions will follow straight
  lines until they hit.
\item Taking the derivative of the optimality condition with respect
  to $\lambda$ leads to the following expression for the velocity of
  the initial clusters:
$$v_i = \sum_{j\neq i}w_{ij}\sign(\alpha_j-\alpha_i)$$
\item When 2 clusters $C_1$ and $C_2$ hit, they will merge to form a
  new cluster $C = C_1\cup C_2$ and take a new velocity:
$$v_C = \frac{
|C_1|v_1 + |C_2|v_2
}{
|C_1|+|C_2|
}$$
\item Stop when all the points merge at the mean $\overline X$.
\item Combine dimensions using $\lambda$ values.
\end{enumerate}

\end{block}

\input{l1-2d}\input{l1-dims}


\end{column}\hfill
\begin{column}{0.315\linewidth}
\begin{alertblock}{Code!
    \textbf{\url{http.//clusterpath.r-forge.r-project.org/}}}
\begin{itemize}
\item Dedicated C++ optimization algorithms with R interface.
  \begin{itemize}
  \item Path algorithm for $\ell_1$ norm with identity weights.
  \item Active-set descent algorithm for $\ell_2$ problem.
  \end{itemize}
\item R interface to Python \texttt{cvxmod} clusterpath solver.
\item Clusterpath visualizations in 2d, 3d, and animations.
\item Coming soon: picking the number of clusters automatically!
\end{itemize}
\end{alertblock}



\begin{block}{Experiments}
{\bfseries Clustering 2 moons}
\begin{itemize}
\item $\y = \X\w + \e$; $\X$, background images; $\e$ is sparse and structured.
\item $p \approx 60\,000$; $|\GG| \approx 120\,000$; $|E| \approx 660\,000$; $1.5sec / \text{prox}$
\end{itemize}
   \begin{figure}
      \centering
%      \includegraphics[width=6.6cm]{images/original_trees.png} \hfill
%      \includegraphics[width=6.6cm]{images/background_trees_struct.png}\hfill
%      \includegraphics[width=6.6cm]{images/foreground_trees_L1.png}\hfill 
%      \includegraphics[width=6.6cm]{images/foreground_trees_struct.png}\hfill 
%      \includegraphics[width=6.6cm]{images/foreground_trees_struct_BIS.png} \\
%     \vspace*{0.2cm}
%       \includegraphics[width=6.6cm]{images/original_video_boot.png}\hfill
%       \includegraphics[width=6.6cm]{images/background_video_boot_struct.png}\hfill
%       \includegraphics[width=6.6cm]{images/foreground_video_boot_L1.png}\hfill
%       \includegraphics[width=6.6cm]{images/foreground_video_boot_struct.png}\hfill 
%       \includegraphics[width=6.6cm]{images/foreground_video_boot_struct_BIS.png} \\
    \caption{{\small Original, est. background, est. foreground with $\ell_1$, with $\ell_1+\Omega$.}}
   \end{figure}
\vspace*{-0.3cm}
{\bfseries Multi-task learning of hierarchical structures}
   \begin{displaymath}
    \min_{\X,\W}
    \frac{1}{n}\sum_{i=1}^n\!\Big[\frac{1}{2} \|\y^i-\X\w^i\|_2^2 + \lambda_1 \Omega_{\text{tree}}(\w^i)\Big]\!+\!\lambda_2\Omega_{\text{joint}}(\W),\ \text{s.t.}\
    \forall j,~~\|\x^j\|_2\leq 1, 
\end{displaymath}
$n=10\,000$ image patches; $p,|\GG| \approx 4\,000\,000$; $|E|\approx 12\,000\,000$
\begin{figure}[hbtp]
   \centering
%   \includegraphics[width=0.42\linewidth]{images/tree2.png}\hfill
%   \includegraphics[width=0.54\linewidth]{images/tree_denois.png} 
   \caption{Example of hierarchy - Mean square error versus dictionary size. 
   } \label{fig:tree}
\end{figure}
\end{block}
\includegraphics[width=0.3\paperwidth]{moons}

\includegraphics{iris-clusterpath}
{\small
\input{moons-iris}
}


\end{column}
\end{columns}

\end{frame}
\end{document}

